# methods/infer_response.py

def generate_response_stream(model, prompt, max_tokens=256, temperature=0.6, stop=None):
    """
    Generator function to yield streaming responses from the model.

    Parameters:
    - model: Loaded model instance (must support calling with stream=True).
    - prompt: The text prompt string to generate from.
    - max_tokens: Maximum tokens to generate.
    - temperature: Sampling temperature.
    - stop: List of stop tokens.

    Yields:
    - token strings incrementally generated by the model.
    """
    if stop is None:
        stop = ["\nUser:", "\nAssistant:"]

    for output in model(prompt, max_tokens=max_tokens, temperature=temperature, stop=stop, stream=True):
        token = output.get("choices", [{}])[0].get("text", "")
        if token:
            yield token


def generate_response(model, prompt, max_tokens=256, temperature=0.6, stop=None):
    """
    Function to generate the full response from the model (non-streaming).

    Parameters:
    - model: Loaded model instance (callable).
    - prompt: The text prompt string to generate from.
    - max_tokens: Maximum tokens to generate.
    - temperature: Sampling temperature.
    - stop: List of stop tokens.

    Returns:
    - Full generated text string.
    """
    if stop is None:
        stop = ["\nUser:", "\nAssistant:"]

    output = model(prompt, max_tokens=max_tokens, temperature=temperature, stop=stop)
    generated_text = output.get("choices", [{}])[0].get("text", "")
    return generated_text